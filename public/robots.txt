# robots.txt
# Allow all crawlers
User-agent: *
Allow: /

# Disallow private/protected routes
Disallow: /api/
Disallow: /dashboard/
Disallow: /login/
Disallow: /*/dashboard/
Disallow: /*/login/

# Sitemap location
Sitemap: https://your-domain.com/sitemap.xml

# Crawl-delay (optional, adjust if needed)
# Crawl-delay: 10
